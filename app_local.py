import os
import sys
import json
import tempfile
from typing import List, Dict, Any

import streamlit as st
import threading
from streamlit.runtime.scriptrunner import add_script_run_ctx

# Add the sop_generator directory to Python path for local development
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'sop_generator'))

from agents import (
    build_coordinator,
    build_sop_generator,
    build_document_parser,
    build_content_styler,
    build_critic,
    build_quality_checker,
    build_safety_agent,
    build_generation_instruction,
    orchestrate_workflow,
    summarize_parsed_chunks,
)
from agents.coordinator import iterative_generate_until_approved
from utils.document_processor import parse_documents_to_chunks
from utils.template_manager import load_template, apply_styles
from utils.export_manager import populate_docx, export_to_docx, export_to_pdf

APP_TITLE = "SOP Generator (AutoGen + Streamlit)"


def init_session_state() -> None:
    if "meta" not in st.session_state:
        st.session_state.meta = {
            "title": "",
            "number": "",
            "equipment": "",
        }
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "sections" not in st.session_state:
        st.session_state.sections = []  # [{title, mode, prompt, content}]
    if "logs" not in st.session_state:
        st.session_state.logs = []
    if "parsed_chunks" not in st.session_state:
        st.session_state.parsed_chunks = []
    if "preview" not in st.session_state:
        st.session_state.preview = []  # sections with content
    if "running" not in st.session_state:
        st.session_state.running = False
    if "worker" not in st.session_state:
        st.session_state.worker = None
    if "conversation" not in st.session_state:
        st.session_state.conversation = []  # [{sender, content}]


def add_log(message: str) -> None:
    # Be resilient when called from a background thread
    try:
        if "logs" not in st.session_state:
            st.session_state.logs = []
        st.session_state.logs.append(message)
        st.session_state.logs = st.session_state.logs[-500:]
    except Exception:
        # Best-effort: ignore logging errors from threads without context
        pass


def ui_home():
    st.header("–ì–ª–∞–≤–Ω–∞—è")
    col1, col2 = st.columns(2)
    with col1:
        st.session_state.meta["title"] = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –°–û–ü", value=st.session_state.meta.get("title", ""))
        st.session_state.meta["number"] = st.text_input("–ù–æ–º–µ—Ä –°–û–ü", value=st.session_state.meta.get("number", ""))
        st.session_state.meta["equipment"] = st.text_input("–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è", value=st.session_state.meta.get("equipment", ""))
    with col2:
        st.markdown("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (PDF/DOCX/Excel)")
        uploads = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã", type=["pdf", "docx", "xlsx", "xls"], accept_multiple_files=True)
        if uploads:
            tmpdir = tempfile.mkdtemp(prefix="sop_docs_")
            paths = []
            for uf in uploads:
                p = os.path.join(tmpdir, uf.name)
                with open(p, "wb") as f:
                    f.write(uf.getbuffer())
                paths.append(p)
            st.session_state.uploaded_files = paths
            st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(paths)}")

    st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", key="structure_hint")


def ui_sections():
    st.header("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–æ–≤")
    if st.button("–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª"):
        st.session_state.sections.append({
            "title": f"–†–∞–∑–¥–µ–ª {len(st.session_state.sections)+1}",
            "mode": "ai",
            "prompt": "",
            "content": "",
        })
    for idx, section in enumerate(st.session_state.sections):
        with st.expander(f"{idx+1}. {section['title']}", expanded=False):
            section["title"] = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ", value=section["title"], key=f"title_{idx}")
            section["mode"] = st.selectbox(
                "–†–µ–∂–∏–º",
                options=["ai", "ai+doc", "manual"],
                index=["ai", "ai+doc", "manual"].index(section["mode"]),
                key=f"mode_{idx}"
            )
            section["prompt"] = st.text_area("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ–¥—Å–∫–∞–∑–∫–∞)", value=section.get("prompt", ""), key=f"prompt_{idx}")
            
            # Add document upload for ai+doc mode
            if section["mode"] == "ai+doc":
                st.markdown("üìÅ **–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞**")
                uploads = st.file_uploader(
                    f"–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞ '{section['title']}'", 
                    type=["pdf", "docx", "xlsx", "xls", "txt"],
                    accept_multiple_files=True,
                    key=f"section_docs_{idx}",
                    help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –ò–ò –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞"
                )
                if uploads:
                    import tempfile
                    tmpdir = tempfile.mkdtemp(prefix=f"sop_section_{idx}_")
                    paths = []
                    for uf in uploads:
                        p = os.path.join(tmpdir, uf.name)
                        with open(p, "wb") as f:
                            f.write(uf.getbuffer())
                        paths.append(p)
                    section["documents"] = paths
                    st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(paths)} - {', '.join([uf.name for uf in uploads])}")
                else:
                    section["documents"] = section.get("documents", [])
                    
            elif section["mode"] == "manual":
                section["content"] = st.text_area("–ö–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–∞", value=section.get("content", ""), height=200, key=f"content_{idx}")
        st.session_state.sections[idx] = section


def ui_generate():
    st.header("–ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", type="primary", disabled=st.session_state.running):
            if st.session_state.worker and st.session_state.worker.is_alive():
                add_log("–£–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∑–∞–¥–∞—á–∞. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")
            else:
                st.session_state.running = True
                t = threading.Thread(target=run_generation_safe, daemon=True)
                add_script_run_ctx(t)
                st.session_state.worker = t
                t.start()
        if st.button("–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", help="–ó–∞–≥–ª—É—à–∫–∞ ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞", disabled=not st.session_state.running):
            add_log("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ (–∑–∞–≥–ª—É—à–∫–∞)")
    with col2:
        st.progress(min(len(st.session_state.preview) / max(len(st.session_state.sections), 1), 1.0))

    st.subheader("–õ–æ–≥ –∞–≥–µ–Ω—Ç–æ–≤")
    st.text("\\n".join(st.session_state.logs[-50:]))
    # Auto-refresh hint for long runs
    if st.session_state.running:
        st.caption("–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è... –û–±–Ω–æ–≤–∏—Ç–µ –≤–∫–ª–∞–¥–∫—É –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –º–µ–∂–¥—É –≤–∫–ª–∞–¥–∫–∞–º–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ª–æ–≥–æ–≤.")


def ui_preview_and_export():
    st.header("–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä")
    st.session_state.preview = st.session_state.preview or [
        {"title": s["title"], "content": s.get("content", "")} for s in st.session_state.sections
    ]
    for idx, sec in enumerate(st.session_state.preview):
        with st.expander(f"{idx+1}. {sec['title']}", expanded=False):
            # Show formatted preview
            content = sec.get("content", "")
            if content:
                # Render markdown content with better table support
                st.markdown("**–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä:**")
                st.markdown(content, unsafe_allow_html=False)
                st.divider()
            # Allow editing in text area
            st.markdown("**–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**")
            sec["content"] = st.text_area("–ö–æ–Ω—Ç–µ–Ω—Ç (Markdown –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)", value=content, height=220, key=f"prev_content_{idx}", help="–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Markdown –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –¢–∞–±–ª–∏—Ü—ã: | –ó–∞–≥–æ–ª–æ–≤–æ–∫ | –ó–∞–≥–æ–ª–æ–≤–æ–∫ | \\n |-------|-------|")
        st.session_state.preview[idx] = sec

    st.subheader("–≠–∫—Å–ø–æ—Ä—Ç")
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("–≠–∫—Å–ø–æ—Ä—Ç –≤ Word"):
            doc, styles = load_template(os.path.join("sop_generator", "templates"))
            apply_styles(doc, styles)
            doc = populate_docx(doc, st.session_state.meta, st.session_state.preview)
            out_path = export_to_docx(doc, os.path.join(tempfile.gettempdir(), "sop_generated.docx"))
            st.success(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_path}")
            with open(out_path, "rb") as f:
                st.download_button("–°–∫–∞—á–∞—Ç—å DOCX", f, file_name="sop_generated.docx")
    with col2:
        if st.button("–≠–∫—Å–ø–æ—Ä—Ç –≤ PDF"):
            out_path = export_to_pdf(st.session_state.preview, os.path.join(tempfile.gettempdir(), "sop_generated.pdf"), st.session_state.meta)
            st.success(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_path}")
            with open(out_path, "rb") as f:
                st.download_button("–°–∫–∞—á–∞—Ç—å PDF", f, file_name="sop_generated.pdf")
    with col3:
        if st.button("–≠–∫—Å–ø–æ—Ä—Ç –≤ Markdown (docs/)"):
            docs_dir = os.path.join("docs")
            os.makedirs(docs_dir, exist_ok=True)
            md_path = os.path.abspath(os.path.join(docs_dir, "sop_generated.md"))
            with open(md_path, "w", encoding="utf-8") as mf:
                mf.write(f"# {st.session_state.meta.get('title','–°–û–ü')}\\n\\n")
                if st.session_state.meta.get("number"):
                    mf.write(f"–ù–æ–º–µ—Ä: {st.session_state.meta['number']}\\n\\n")
                for idx, sec in enumerate(st.session_state.preview, start=1):
                    mf.write(f"## {idx}. {sec['title']}\\n\\n{sec.get('content','')}\\n\\n")
            st.success(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {md_path}")


def run_mock_generation():
    """Generate SOP via a multi-round MOCK group chat using uploaded docs and section prompts."""
    add_log("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –°–û–ü —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏ (–º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤—ã–π –¥–∏–∞–ª–æ–≥)...")
    
    # Conversation transcript holder
    st.session_state.conversation = []
    def say(sender: str, content: str):
        st.session_state.conversation.append({"sender": sender, "content": content})
        add_log(f"{sender}: {content[:120]}{'...' if len(content)>120 else ''}")
    
    # Ingest docs (global + per-section ai+doc)
    all_docs = st.session_state.uploaded_files.copy() if st.session_state.uploaded_files else []
    for section in st.session_state.sections:
        if section.get("mode") == "ai+doc" and section.get("documents"):
            all_docs.extend(section["documents"])
    chunks = parse_documents_to_chunks(all_docs)
    corpus_summary = summarize_parsed_chunks(chunks)
    
    title = st.session_state.meta.get("title", "–¢–µ—Å—Ç–æ–≤—ã–π –°–û–ü")
    number = st.session_state.meta.get("number", "TEST-001")
    equipment = st.session_state.meta.get("equipment", "–¢–µ—Å—Ç–æ–≤–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ")
    
    # Build a simple section outline from configured sections
    cfg_titles = [s.get("title", f"–†–∞–∑–¥–µ–ª {i+1}") for i, s in enumerate(st.session_state.sections)]
    outline_md = "\n".join([f"## {i+1}. {t}" for i, t in enumerate(cfg_titles, start=1)])
    prompts_md = "\n".join([f"- {i+1}. {s.get('title','')}: {s.get('prompt','') or '‚Äî'}" for i, s in enumerate(st.session_state.sections)])
    corpus_md = ""
    
    # Round loop
    max_iters = 3
    draft = f"# {title}\n–ù–æ–º–µ—Ä: {number}\n\n{outline_md}\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n- –£–∫–∞–∑–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏ –∏ QC\n- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\n\n–£–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{prompts_md}{corpus_md}"
    
    for it in range(1, max_iters+1):
        say("SOP_Generator", f"–ò—Ç–µ—Ä–∞—Ü–∏—è {it}: —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–Ω–æ–≤–∏–∫ —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
        # Expand sections using heuristics + doc summary
        body = []
        for idx, sec in enumerate(st.session_state.sections):
            sec_title = sec.get("title", f"–†–∞–∑–¥–µ–ª {idx+1}")
            prompt = sec.get("prompt", "")
            base = ""
            lt = sec_title.lower()
            if any(k in lt for k in ["–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω", "–æ–±—É—á–µ–Ω"]):
                base = """–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª:\n- –°—Ç–∞—Ä—à–∏–π –ª–∞–±–æ—Ä–∞–Ω—Ç ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã\n- –õ–∞–±–æ—Ä–∞–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–æ–≤\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø—Ä–æ—Ñ–∏–ª—å–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç, —Å—Ç–∞–∂ ‚â•2 –ª–µ—Ç.\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –æ–±—É—á–µ–Ω–∏—è: –≤–≤–æ–¥–Ω—ã–π –∫—É—Ä—Å + –µ–∂–µ–≥–æ–¥–Ω–∞—è –ø–µ—Ä–µ–∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è."""
            elif any(k in lt for k in ["—Ü–µ–ª—å", "–æ–±–ª–∞—Å—Ç—å"]):
                base = f"""–î–∞–Ω–Ω–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Ä–∞–±–æ—Ç—ã —Å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º {equipment}.\n–û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ —Ä–∞–±–æ—Ç—ã.\n–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –≤–Ω–µ —Ä–∞–º–æ–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è.\n–ò—Å–∫–ª—é—á–µ–Ω–∏—è: –æ—Å–æ–±—ã–µ –æ–ø–∞—Å–Ω—ã–µ –≤–µ—â–µ—Å—Ç–≤–∞ —Ç—Ä–µ–±—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –°–û–ü."""
            elif any(k in lt for k in ["—Ä–∏—Å–∫", "–±–µ–∑–æ–ø–∞—Å"]):
                base = """–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: —Ö–∏–º–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ, —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —à–æ–∫, –º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–≤–º—ã.\n–ú–µ—Ä—ã –∑–∞—â–∏—Ç—ã: –°–ò–ó (–æ—á–∫–∏, –ø–µ—Ä—á–∞—Ç–∫–∏, —Ö–∞–ª–∞—Ç), –≤—ã—Ç—è–∂–Ω–æ–π —à–∫–∞—Ñ, –∞–≤–∞—Ä–∏–π–Ω—ã–π –¥—É—à.\n–í–ù–ò–ú–ê–ù–ò–ï: –ø—Ä–∏ —É—Ç–µ—á–∫–∞—Ö/–∑–∞–ø–∞—Ö–µ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞–±–æ—Ç—É."""
            elif any(k in lt for k in ["–æ–±–æ—Ä—É–¥", "–º–∞—Ç–µ—Ä–∏–∞–ª"]):
                base = f"""–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {equipment}.\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å—ã, –º–µ—Ä–Ω–∞—è –ø–æ—Å—É–¥–∞.\n–†–µ–∞–≥–µ–Ω—Ç—ã: —Ä–∞—Å—Ç–≤–æ—Ä–∏—Ç–µ–ª–∏ –≤—ã—Å–æ–∫–æ–π —á–∏—Å—Ç–æ—Ç—ã, —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã.\n–•—Ä–∞–Ω–µ–Ω–∏–µ: 15‚Äì25¬∞C, RH<60%."""
            elif any(k in lt for k in ["–ø—Ä–æ—Ü–µ–¥—É—Ä", "—à–∞–≥"]):
                base = """1) –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –∏ –ø—Ä–æ–≥—Ä–µ—Ç—å.\n2) –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞: 1‚Äì100 –º–≥/–ª, R¬≤>0.995.\n3) –ê–Ω–∞–ª–∏–∑: –æ–±—ä—ë–º 1.0 –º–ª, –≤—Ä–µ–º—è 15 –º–∏–Ω, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ ‚Äî 3.\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞: RSD<5%."""
            elif any(k in lt for k in ["–∫–æ–Ω—Ç—Ä–æ–ª—å", "quality", "qc"]):
                base = """–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞: —Ö–æ–ª–æ—Å—Ç–∞—è –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –ø—Ä–æ–±—ã, –∫–æ–Ω—Ç—Ä–æ–ª—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ ¬±10%, –¥—Ä–µ–π—Ñ<5%. –ß–∞—Å—Ç–æ—Ç–∞: –µ–∂–µ–¥–Ω–µ–≤–Ω–æ."""
            elif any(k in lt for k in ["–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç", "–∑–∞–ø–∏—Å", "–∂—É—Ä–Ω–∞–ª"]):
                base = """–ñ—É—Ä–Ω–∞–ª—ã: —Ä–∞–±–æ—Ç—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –∞–Ω–∞–ª–∏–∑–æ–≤, –∂—É—Ä–Ω–∞–ª QC.\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ—Å—Ç—å, –∑–∞–≤–µ—Ä–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π, —Å—Ä–æ–∫–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è 3‚Äì5 –ª–µ—Ç."""
            elif any(k in lt for k in ["—Å—Å—ã–ª–∫", "–Ω–æ—Ä–º–∞—Ç–∏–≤", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç"]):
                base = """–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å—Å—ã–ª–∫–∏: ISO/IEC 17025, –ì–û–°–¢—ã –ø–æ –º–µ—Ç–æ–¥–∞–º, –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏."""
            elif any(k in lt for k in ["–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω", "troubleshooting", "—Å–∏–º–ø—Ç–æ–º"]):
                base = """–°–∏–º–ø—Ç–æ–º‚Üí–ü—Ä–∏—á–∏–Ω–∞‚Üí–î–µ–π—Å—Ç–≤–∏–µ: –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª‚Üí–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞‚Üí–æ—á–∏—Å—Ç–∏—Ç—å; –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–∏–≥–Ω–∞–ª–∞‚Üí–∏—Å—Ç–æ—á–Ω–∏–∫/–∫–∞–±–µ–ª—å‚Üí–ø—Ä–æ–≤–µ—Ä–∏—Ç—å/–∑–∞–º–µ–Ω–∏—Ç—å."""
            else:
                base = ""
            # Merge doc cues (removed from output)
            body.append(f"## {idx+1}. {sec_title}\n{base}")
        draft = f"# {title}\n–ù–æ–º–µ—Ä: {number}\n\n" + "\n\n".join(body)
        
        # Safety agent
        say("Safety_Agent", "–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É—é –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏, –°–ò–ó –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ä–∞–∑–¥–µ–ª—ã.")
        if "–í–ù–ò–ú–ê–ù–ò–ï" not in draft:
            draft += "\n\n–í–ù–ò–ú–ê–ù–ò–ï: —Å–æ–±–ª—é–¥–∞—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ä–µ–∞–≥–µ–Ω—Ç–∞–º–∏ –∏ –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º."
        if "–°–ò–ó" not in draft:
            draft += "\n\n–°–ò–ó: –∑–∞—â–∏—Ç–Ω—ã–µ –æ—á–∫–∏, –ø–µ—Ä—á–∞—Ç–∫–∏, —Ö–∞–ª–∞—Ç; –ø—Ä–∏ –ª–µ—Ç—É—á–∏—Ö ‚Äî —Ä–∞–±–æ—Ç–∞ –ø–æ–¥ –≤—ã—Ç—è–∂–∫–æ–π."
        
        # Quality checker
        say("Quality_Checker", "–ü—Ä–æ–≤–µ—Ä—è—é –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏ –∏ QC: –¥–æ–±–∞–≤–ª—è—é —è–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –∏ —á–∞—Å—Ç–æ—Ç—ã.")
        if "–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏" not in draft:
            draft += "\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏: R¬≤>0.995; RSD<5%; –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π –ø—Ä–æ–±—ã ¬±10%."
        if "–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞" not in draft:
            draft += "\n\n–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞: —Ö–æ–ª–æ—Å—Ç–∞—è –ø—Ä–æ–±–∞ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è ‚Äî –∫–∞–∂–¥—ã–µ 10 –∞–Ω–∞–ª–∏–∑–æ–≤."
        
        # Critic
        issues = []
        if "–¶–µ–ª—å" not in draft and "–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è" not in draft.lower():
            issues.append("–ù–µ—Ç —è–≤–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Ü–µ–ª–∏/–æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.")
        if "–°–ò–ó" not in draft:
            issues.append("–°–ª–∞–±–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –°–ò–ó.")
        if "–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏" not in draft:
            issues.append("–ù–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –ø—Ä–∏—ë–º–∫–∏.")
        status = "APPROVED" if not issues or it == max_iters else "REVISE"
        say("Critic", f"SUMMARY: –ò—Ç–µ—Ä–∞—Ü–∏—è {it}.\nISSUES: {('; '.join(issues) or '–Ω–µ—Ç')}\nSTATUS: {status}")
        
        if status == "APPROVED":
            say("Styler", "–ü—Ä–∏–≤–æ–∂—É —Ç–µ–∫—Å—Ç –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∏–ª—é –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è).")
            break
        else:
            say("Coordinator", "–ü—Ä–∏–Ω—è—Ç–æ. –í–Ω–µ—Å—ë–º –ø—Ä–∞–≤–∫–∏ –∏ –ø–æ–≤—Ç–æ—Ä–∏–º –∏—Ç–µ—Ä–∞—Ü–∏—é.")
    
    # Final content is current 'draft'
    final_text = draft
    
    # Parse to preview sections
    def parse_sections_from_content(content: str, section_configs: list) -> list:
        # Reuse the robust parser defined in run_generation() below by calling it dynamically
        # If not available yet, perform minimal split by '## N.'
        import re
        parts = re.split(r"\n(?=##\s*\d+\.)", content)
        result = []
        if len(parts) <= 1:
            # Fallback to one section
            for s in section_configs:
                result.append({"title": s.get("title","–†–∞–∑–¥–µ–ª"), "content": content})
            return result
        # Map parts by order
        body_sections = [p.strip() for p in parts if p.strip() and p.strip().startswith("##")]
        for idx, sc in enumerate(section_configs):
            if idx < len(body_sections):
                # strip heading line from content
                lines = body_sections[idx].split("\n", 1)
                content_i = lines[1] if len(lines) > 1 else ""
                result.append({"title": sc.get("title", f"–†–∞–∑–¥–µ–ª {idx+1}"), "content": content_i.strip()})
            else:
                result.append({"title": sc.get("title", f"–†–∞–∑–¥–µ–ª {idx+1}"), "content": ""})
        return result
    
    st.session_state.preview = parse_sections_from_content(final_text, st.session_state.sections)
    add_log("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤—ã–π –¥–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞–∑–¥–µ–ª–æ–≤ –æ–±–Ω–æ–≤–ª—ë–Ω.")


def run_generation_safe():
    try:
        run_generation()
    except Exception as e:
        add_log(f"–û—à–∏–±–∫–∞: {e}")
    finally:
        st.session_state.running = False


def run_generation():
    add_log("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤...")
    
    # Check if we should use mock mode (no API key or mock flag)
    use_mock_mode = not os.getenv("API_KEY") or os.getenv("USE_MOCK_MODE", "").lower() == "true"
    
    if use_mock_mode:
        add_log("üîß –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        run_mock_generation()
        return
    
    coord = build_coordinator(on_log=add_log)
    sop_gen = build_sop_generator()
    doc_parser = build_document_parser()
    styler = build_content_styler()
    critic = build_critic()
    quality = build_quality_checker()
    safety = build_safety_agent()

    add_log("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    # Process global documents
    all_docs = st.session_state.uploaded_files.copy() if st.session_state.uploaded_files else []
    
    # Add section-specific documents
    for section in st.session_state.sections:
        if section.get("mode") == "ai+doc" and section.get("documents"):
            all_docs.extend(section["documents"])
    
    chunks = parse_documents_to_chunks(all_docs)
    st.session_state.parsed_chunks = chunks
    corpus_summary = summarize_parsed_chunks(chunks)
    
    add_log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_docs)} (–≥–ª–æ–±–∞–ª—å–Ω—ã—Ö: {len(st.session_state.uploaded_files or [])}, –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: {len(all_docs) - len(st.session_state.uploaded_files or [])})")

    def base_instruction_builder(critique: str) -> str:
        return build_generation_instruction(
            sop_title=st.session_state.meta["title"],
            sop_number=st.session_state.meta["number"],
            equipment_type=st.session_state.meta["equipment"],
            sections=st.session_state.sections,
            parsed_corpus_summary=corpus_summary if corpus_summary else None,
            critique_feedback=critique or None,
        )

    add_log("–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ –æ–¥–æ–±—Ä–µ–Ω–∏—è –∫—Ä–∏—Ç–∏–∫–æ–º...")
    loop_result = iterative_generate_until_approved(
        coordinator=coord,
        sop_gen=sop_gen,
        safety=safety,
        critic=critic,
        quality=quality,
        styler=styler,
        base_instruction_builder=base_instruction_builder,
        max_iters=4,  # Increased for better refinement
        logger=add_log,
    )

    add_log("–°–±–æ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–æ–≤...")
    generated_clean_content = loop_result.get("content", "")
    
    # Parse sections from the clean content
    def parse_sections_from_content(content: str, section_configs: list) -> list:
        """Parse the generated content into individual sections with robust rules."""
        if not content.strip():
            return [{"title": s["title"], "content": "–ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"} for s in section_configs]
        
        # Pre-filter: drop obvious instruction/spec blocks
        filtered_lines = []
        skip_spec_block = False
        for raw_line in content.split('\n'):
            line = raw_line.strip()
            line_lower = line.lower()
            if any(k in line_lower for k in [
                "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ - –∏–∑–±–µ–≥–∞–π", "–¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–æ–≤", 
                "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:", "—à–∞–±–ª–æ–Ω –¥–ª—è —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞:",
                "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞", "—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π"
            ]):
                skip_spec_block = True
                continue
            # End skip when we reach a real section header
            if line.startswith('#') or line_lower.startswith('—Åop-') or line_lower.startswith('—Å–æ–ø-'):
                skip_spec_block = False
            if skip_spec_block:
                continue
            # Also skip lines like "–†–ê–ó–î–ï–õ X:" which are specs, not content
            if line.upper().startswith("–†–ê–ó–î–ï–õ "):
                continue
            filtered_lines.append(raw_line)
        text = '\n'.join(filtered_lines)
        
        # Split content by section headers
        sections_parsed = []
        lines = text.split('\n')
        current_section = None
        current_content: list[str] = []
        
        # Header patterns (exclude plain numeric to avoid capturing spec preambles)
        import re
        header_patterns = [
            re.compile(r'^#{1,6}\s*\d+\.?\s*(.+)$'),   # ## 1. Title
            re.compile(r'^#{1,6}\s*(.+)$'),              # ## Title
            re.compile(r'^\*\*\d+\.?\s*(.+)\*\*$'), # **1. Title**
            re.compile(r'^\s*–†–ê–ó–î–ï–õ\s+\d+[:\-. ]\s*(.+)$', re.IGNORECASE), # –†–ê–ó–î–ï–õ 1: Title
        ]
        
        def is_header(s: str) -> tuple[bool, str | None]:
            for pat in header_patterns:
                m = pat.match(s.strip())
                if m:
                    title = m.group(1).strip()
                    # Normalize bold markers
                    title = re.sub(r'\*+', '', title)
                    return True, title
            return False, None
        
        for raw in lines:
            stripped = raw.strip()
            match, header_title = is_header(stripped)
            if match and header_title:
                if current_section and current_content:
                    sections_parsed.append({
                        "title": current_section,
                        "content": '\n'.join(current_content).strip()
                    })
                current_section = header_title
                current_content = []
                continue
            if current_section is not None:
                current_content.append(raw)
        
        if current_section and current_content:
            sections_parsed.append({
                "title": current_section,
                "content": '\n'.join(current_content).strip()
            })
        
        # If nothing parsed, fallback to whole text as a single section to avoid duplication logic
        if not sections_parsed:
            sections_parsed = [{"title": "–î–æ–∫—É–º–µ–Ω—Ç", "content": text.strip()}]
        
        # Drop document preamble if present (title block with only metadata like "–ù–æ–º–µ—Ä:")
        if sections_parsed:
            first_block = sections_parsed[0]
            fb_text = (first_block.get("title", "") + "\n" + first_block.get("content", "")).lower()
            if ("–Ω–æ–º–µ—Ä:" in fb_text or "sop" in fb_text or "—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü" in fb_text):
                # Consider it a preamble, not a real section
                sections_parsed = sections_parsed[1:] if len(sections_parsed) > 1 else sections_parsed
        
        # Heuristic classifiers (defined locally for mock parser)
        canonical_titles = {
            "scope": "–¶–µ–ª—å –∏ –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
            "responsibility": "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ –æ–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∞",
            "safety": "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
            "equipment": "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã",
            "procedure": "–ü–æ—à–∞–≥–æ–≤—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã",
            "quality": "–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞",
            "records": "–î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –∏ –≤–µ–¥–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π",
            "references": "–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å—Å—ã–ª–∫–∏",
            "troubleshooting": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π",
        }
        
        def guess_key_from_text(text_: str) -> str | None:
            t = text_.lower()
            if any(k in t for k in ["—Ü–µ–ª—å", "–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω", "–∏—Å–∫–ª—é—á–µ–Ω", "–≥—Ä–∞–Ω–∏—Ü", "–ø–æ–¥–≥–æ—Ç–æ–≤–∫"]):
                return "scope"
            if any(k in t for k in ["–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω", "–æ–±—É—á–µ–Ω", "–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü", "–¥–æ–ø—É—Å–∫", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "—Ä–æ–ª—å"]):
                return "responsibility"
            if any(k in t for k in ["—Ä–∏—Å–∫", "–æ–ø–∞—Å–Ω–æ—Å—Ç", "–±–µ–∑–æ–ø–∞—Å", "—Å–∏–∑", "–ª–æ—Ç–æ", "–≤–Ω–∏–º–∞–Ω–∏", "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥"]):
                return "safety"
            if any(k in t for k in ["–æ–±–æ—Ä—É–¥", "–º–∞—Ç–µ—Ä–∏–∞–ª", "–∫–æ–ª–æ–Ω–∫", "–¥–µ—Ç–µ–∫—Ç–æ—Ä", "—Ä–µ–∞–≥–µ–Ω—Ç", "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü"]):
                return "equipment"
            if any(k in t for k in ["—à–∞–≥", "–ø—Ä–æ—Ü–µ–¥—É—Ä", "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å", "–∏–Ω—Å—Ç—Ä—É–∫—Ü", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä", "–≤—Ä–µ–º—è", "–æ–±—ä–µ–º", "—Å–∫–æ—Ä–æ—Å—Ç", "–¥–∞–≤–ª–µ–Ω"]):
                return "procedure"
            if any(k in t for k in ["–∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ", "qc", "–∫—Ä–∏—Ç–µ—Ä–∏", "–ø—Ä–∏–µ–º–∫", "–≤–∞–ª–∏–¥–∞—Ü", "rsd", "–¥–æ–ø—É—Å–∫"]):
                return "quality"
            if any(k in t for k in ["–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç", "–∑–∞–ø–∏—Å", "–∂—É—Ä–Ω–∞–ª", "–æ—Ç—á–µ—Ç", "—Ñ–æ—Ä–º–∞—Ç", "—Å—Ä–æ–∫ —Ö—Ä–∞–Ω"]):
                return "records"
            if any(k in t for k in ["—Å—Å—ã–ª–∫", "–Ω–æ—Ä–º–∞—Ç–∏–≤", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "gost", "iso"]):
                return "references"
            if any(k in t for k in ["–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω", "—Å–∏–º–ø—Ç–æ–º", "–ø—Ä–∏—á–∏–Ω", "–¥–µ–π—Å—Ç–≤", "–¥–∏–∞–≥–Ω–æ—Å—Ç", "—É—Å—Ç—Ä–∞–Ω–µ–Ω"]):
                return "troubleshooting"
            return None
        
        # Compute guess for parsed sections
        parsed_with_guess = []
        for ps in sections_parsed:
            key = guess_key_from_text(ps["title"] + "\n" + ps["content"])
            parsed_with_guess.append({**ps, "_key": key})
        
        # Map parsed sections to configured sections (consume parsed sections once)
        result_sections = []
        used_indices: set[int] = set()
        for idx, section_config in enumerate(section_configs):
            cfg_title = section_config["title"]
            cfg_prompt = section_config.get("prompt", "")
            if section_config.get("mode") == "manual" and section_config.get("content"):
                result_sections.append({"title": cfg_title, "content": section_config["content"]})
                continue
            
            # Determine intended key from config title/prompt
            intended_key = guess_key_from_text((cfg_title + "\n" + cfg_prompt))
            
            matched_content = ""
            chosen_idx: int | None = None
            # 1) Match by intended key
            if intended_key:
                for i, ps in enumerate(parsed_with_guess):
                    if i in used_indices:
                        continue
                    if ps["_key"] == intended_key and ps.get("content"):
                        matched_content = ps["content"]
                        chosen_idx = i
                        break
            # 2) Title similarity fallback
            if not matched_content:
                config_title_lower = cfg_title.lower()
                for i, ps in enumerate(sections_parsed):
                    if i in used_indices:
                        continue
                    if config_title_lower and (config_title_lower in ps["title"].lower() or ps["title"].lower() in config_title_lower):
                        matched_content = ps["content"]
                        chosen_idx = i
                        break
            # 3) Order-based fallback (use next unused by order)
            if not matched_content:
                for i in range(len(sections_parsed)):
                    if i not in used_indices:
                        matched_content = sections_parsed[i]["content"]
                        chosen_idx = i
                        break
                if not matched_content and sections_parsed:
                    matched_content = sections_parsed[-1]["content"]
                    chosen_idx = len(sections_parsed) - 1
            
            if chosen_idx is not None:
                used_indices.add(chosen_idx)
            
            # Auto-rename generic titles to canonical when we know the intent
            result_title = cfg_title
            if re.match(r'^\s*–†–∞–∑–¥–µ–ª\s+\d+\s*$', cfg_title, flags=re.IGNORECASE) and intended_key:
                result_title = canonical_titles.get(intended_key, cfg_title)
            
            result_sections.append({
                "title": result_title,
                "content": matched_content
            })
        
        return result_sections
    
    st.session_state.preview = parse_sections_from_content(generated_clean_content, st.session_state.sections)

    add_log("–ì–æ—Ç–æ–≤–æ. –°—Ç–∞—Ç—É—Å: " + ("–û–¥–æ–±—Ä–µ–Ω–æ" if loop_result.get("approved") else "–ù—É–∂–Ω—ã –ø—Ä–∞–≤–∫–∏"))


def _run_agent_and_get_messages_local(agent, task: str):
    """Local helper to run an agent and return a list of messages [{sender, content}]."""
    try:
        # Some agent implementations expose async run, others sync generate_reply
        if hasattr(agent, "run"):
            import asyncio
            async def _runner():
                try:
                    result = await agent.run(task=task)
                    msgs = getattr(result, "messages", [])
                    # Normalize
                    out = []
                    for m in msgs:
                        sender = getattr(m, "source", getattr(m, "role", agent.__class__.__name__))
                        content = getattr(m, "content", str(m))
                        out.append({"sender": sender, "content": content})
                    # Fallback when no structured messages
                    if not out and hasattr(result, "content"):
                        out = [{"sender": getattr(agent, "name", "Agent"), "content": result.content}]
                    return out
                except Exception as e:
                    return [{"sender": getattr(agent, "name", "Agent"), "content": f"Error: {e}"}]
            msgs = asyncio.run(_runner())
            return msgs
        elif hasattr(agent, "generate_reply"):
            import asyncio
            async def _runner2():
                try:
                    reply = await agent.generate_reply(task)
                    return [{"sender": getattr(agent, "name", "Agent"), "content": reply}]
                except Exception as e:
                    return [{"sender": getattr(agent, "name", "Agent"), "content": f"Error: {e}"}]
            return asyncio.run(_runner2())
        else:
            return [{"sender": getattr(agent, "name", "Agent"), "content": "[No run/generate method available]"}]
    except Exception as e:
        return [{"sender": getattr(agent, "name", "Agent"), "content": f"Execution error: {e}"}]


def run_conversation_preview() -> None:
    """Build agents and run a short conversation to preview interactions."""
    add_log("–ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–∏–∞–ª–æ–≥–∞ –∞–≥–µ–Ω—Ç–æ–≤...")

    # Mock mode ‚Äî no network calls
    use_mock_mode = not os.getenv("API_KEY") or os.getenv("USE_MOCK_MODE", "").lower() == "true"
    if use_mock_mode:
        st.session_state.conversation = [
            {"sender": "Coordinator", "content": "–ö–æ–ª–ª–µ–≥–∏, —Å—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ —á–µ—Ä–Ω–æ–≤–∏–∫ –°–û–ü –ø–æ –≤–≤–µ–¥–µ–Ω–Ω—ã–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –∏ —Ä–∞–∑–¥–µ–ª–∞–º."},
            {"sender": "SOP_Generator", "content": "–ì–æ—Ç–æ–≤ —á–µ—Ä–Ω–æ–≤–æ–π –≤–∞—Ä–∏–∞–Ω—Ç. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–±–ª—é–¥–µ–Ω–∞, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø–æ–ª–Ω–µ–Ω—ã."},
            {"sender": "Safety_Agent", "content": "–î–æ–±–∞–≤–∏–ª –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –í–ù–ò–ú–ê–ù–ò–ï/–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –°–ò–ó."},
            {"sender": "Quality_Checker", "content": "–ù–∞–π–¥–µ–Ω—ã –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: —É—Ç–æ—á–Ω–∏—Ç—å –¥–æ–ø—É—Å–∫–∏ RSD –∏ —á–∞—Å—Ç–æ—Ç—É QC-–ø—Ä–æ–±."},
            {"sender": "Critic", "content": "SUMMARY: –•–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ.\nISSUES: –£—Ç–æ—á–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏.\nSTATUS: REVISE"},
            {"sender": "Styler", "content": "–ü—Ä–∏–≤–µ–ª —Å—Ç–∏–ª—å –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É, –µ–¥–∏–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ."},
        ]
        add_log("–î–∏–∞–ª–æ–≥ (–º–æ–∫) —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –±–µ–∑ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ LLM.")
        return

    # Initialize agents
    coord = build_coordinator(on_log=add_log)
    sop_gen = build_sop_generator()
    safety = build_safety_agent()
    critic = build_critic()
    quality = build_quality_checker()
    styler = build_content_styler()

    # Aggregate docs: global + ai+doc per-section
    all_docs = st.session_state.uploaded_files.copy() if st.session_state.uploaded_files else []
    for section in st.session_state.sections:
        if section.get("mode") == "ai+doc" and section.get("documents"):
            all_docs.extend(section["documents"])

    chunks = parse_documents_to_chunks(all_docs)
    corpus_summary = summarize_parsed_chunks(chunks)

    def base_instruction_builder(critique: str) -> str:
        return build_generation_instruction(
            sop_title=st.session_state.meta.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
            sop_number=st.session_state.meta.get("number", ""),
            equipment_type=st.session_state.meta.get("equipment", ""),
            sections=st.session_state.sections,
            parsed_corpus_summary=corpus_summary if corpus_summary else None,
            critique_feedback=critique or None,
        )

    # Try real group chat first
    try:
        instructions = base_instruction_builder("")
        messages = orchestrate_workflow(
            coordinator=coord,
            agents=[sop_gen, safety, critic, quality, styler],
            instructions=instructions,
            max_rounds=6,
        )
        st.session_state.conversation = messages
        add_log(f"–î–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
        return
    except Exception as e:
        add_log(f"–ì—Ä—É–ø–ø–æ–≤–æ–π —á–∞—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: {e}")

    # Sequential fallback transcript (short)
    transcript: list[dict] = []
    # 1) Generator produces initial draft
    gen_msgs = _run_agent_and_get_messages_local(sop_gen, base_instruction_builder(""))
    transcript.extend(gen_msgs)
    draft_text = "\n\n".join([m["content"] for m in gen_msgs])

    # 2) Safety review
    safety_msgs = _run_agent_and_get_messages_local(safety, f"–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–∑–¥–µ–ª –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n–¢–ï–ö–°–¢:\n{draft_text[:3000]}")
    transcript.extend(safety_msgs)

    # 3) Quality review
    quality_msgs = _run_agent_and_get_messages_local(quality, f"–í—ã—è–≤–∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫.\n–¢–ï–ö–°–¢:\n{draft_text[:3000]}")
    transcript.extend(quality_msgs)

    # 4) Critic summary
    critic_msgs = _run_agent_and_get_messages_local(critic, f"–û—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç. –í–µ—Ä–Ω–∏ SUMMARY/ISSUES/STATUS.\n–¢–ï–ö–°–¢:\n{draft_text[:3000]}")
    transcript.extend(critic_msgs)

    # 5) Styler pass
    styler_msgs = _run_agent_and_get_messages_local(styler, f"–ü—Ä–∏–≤–µ–¥–∏ –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∏–ª—é. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç.\n–¢–ï–ö–°–¢:\n{draft_text[:3000]}")
    transcript.extend(styler_msgs)

    st.session_state.conversation = transcript
    add_log(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–æ–±—â–µ–Ω–∏–π: {len(transcript)}")


def run_conversation_preview_safe() -> None:
    try:
        run_conversation_preview()
    except Exception as e:
        add_log(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–∏–∞–ª–æ–≥–∞: {e}")


def ui_conversation():
    st.header("–î–∏–∞–ª–æ–≥ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤")
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∏–∞–ª–æ–≥–∞", type="primary"):
            t = threading.Thread(target=run_conversation_preview_safe, daemon=True)
            add_script_run_ctx(t)
            t.start()
            st.info("–î–∏–∞–ª–æ–≥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è... –û–±–Ω–æ–≤–∏—Ç–µ –≤–∫–ª–∞–¥–∫—É —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥.")
    with col2:
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –¥–∏–∞–ª–æ–≥"):
            st.session_state.conversation = []
            st.success("–û—á–∏—â–µ–Ω–æ")

    st.markdown("---")
    if not st.session_state.conversation:
        st.info("–ü–æ–∫–∞ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –≤—ã—à–µ, —á—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å –¥–∏–∞–ª–æ–≥.")
        return

    # Render conversation
    for i, msg in enumerate(st.session_state.conversation):
        sender = msg.get("sender", "agent")
        content = msg.get("content", "")
        with st.expander(f"{i+1}. {sender}", expanded=False):
            st.markdown(content)


def main():
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    init_session_state()

    st.title(APP_TITLE)
    tabs = st.tabs(["–ì–ª–∞–≤–Ω–∞—è", "–†–∞–∑–¥–µ–ª—ã", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è", "–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∏ —ç–∫—Å–ø–æ—Ä—Ç", "–î–∏–∞–ª–æ–≥"]) 

    with tabs[0]:
        ui_home()
    with tabs[1]:
        ui_sections()
    with tabs[2]:
        ui_generate()
    with tabs[3]:
        ui_preview_and_export()
    with tabs[4]:
        ui_conversation()


if __name__ == "__main__":
    main()